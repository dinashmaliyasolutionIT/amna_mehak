# -*- coding: utf-8 -*-
"""amna_Mehak.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a3ZHshpe3WP7Rkev-Zeeeg8u4dVNZt1q
"""

pip install requests beautifulsoup4 pandas lxml

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

BASE_URL = "https://daewooinfo.pk"
ROUTE_PAGES = [
    "/daewoo-lahore-islamabad/",
    "/daewoo-lahore-rawalpindi/",
    "/daewoo-lahore-multan/",
    "/daewoo-lahore-faisalabad/",
    "/daewoo-karachi-lahore/",
    "/daewoo-karachi-islamabad/",
    "/daewoo-multan-rawalpindi/",
    "/daewoo-peshawar-lahore/",
]

def get_soup(url):
    """Fetch page and return BeautifulSoup object."""
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "lxml")
    except Exception as e:
        print(f" Error fetching {url}: {e}")
        return None


def parse_route_page(url):
    """Extract route details from a single Daewoo info page."""
    soup = get_soup(url)
    if not soup:
        return None

    data = {"source_url": url}

    # Try to find heading like "Daewoo Lahore to Islamabad"
    title = soup.find("h1")
    if title:
        text = title.get_text(strip=True)
        if " to " in text.lower():
            parts = text.replace("Daewoo", "").split("to")
            data["origin"] = parts[0].strip()
            data["destination"] = parts[1].strip()
        else:
            data["origin"] = text.strip()
            data["destination"] = ""

    # Find all paragraph or table text blocks
    page_text = soup.get_text(" ", strip=True).lower()

    # Extract times or durations heuristically
    def find_field(keyword):
        for line in page_text.split(" "):
            if keyword in line:
                return line
        return None

    # Try to find duration, first and last departure times
    duration = None
    for word in ["hours", "hr", "hrs", "hour"]:
        if word in page_text:
            # rough pattern
            start = page_text.find(word) - 4
            duration = page_text[start:start+15]
            break

    data["travel_duration"] = duration

    # You can add more refined regex-based parsing later

    return data

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from urllib.parse import urljoin, urlparse

base_url = "https://daewooinfo.pk/"
visited = set()
links_to_visit = [base_url]
route_links = set()

print("Starting full crawl from:", base_url)

# Crawl up to 3 levels deep
for _ in range(3):
    new_links = []
    for url in links_to_visit:
        if url in visited:
            continue
        visited.add(url)
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.find_all("a", href=True):
                href = urljoin(base_url, a["href"])
                # stay within same site
                if base_url not in href:
                    continue
                if href not in visited:
                    new_links.append(href)
                # filter Daewoo route/timing pages
                if any(word in href.lower() for word in ["daewoo", "timing", "schedule"]):
                    route_links.add(href)
        except Exception as e:
            print("", e)
    links_to_visit = list(set(new_links))

print(f"\n Found {len(route_links)} possible route/timing pages\n")

routes_data = []
for url in route_links:
    print(" Scraping:", url)
    try:
        page = requests.get(url, timeout=10)
        page.raise_for_status()
        psoup = BeautifulSoup(page.text, "html.parser")

        title = psoup.find("h1")
        route_title = title.get_text(strip=True) if title else "Unknown Route"

        # Try to extract route names like "Lahore to Islamabad"
        match = re.search(r"([A-Za-z\s]+)\s+(?:to|‚Äì|-)\s+([A-Za-z\s]+)", route_title)
        origin = match.group(1).strip() if match else None
        destination = match.group(2).strip() if match else None

        # Extract timings or table info
        details = []
        for tag in psoup.find_all(["table", "p", "li"]):
            text = tag.get_text(" ", strip=True)
            if any(word in text.lower() for word in ["am", "pm", "departure", "arrival", "duration", "time", "schedule"]):
                details.append(text)

        routes_data.append({
            "url": url,
            "origin": origin,
            "destination": destination,
            "details": " | ".join(details) if details else "No timing info found"
        })

    except Exception as e:
        print(f" Error on {url}: {e}")

df = pd.DataFrame(routes_data)
df.to_csv("daewoo_full_routes.csv", index=False, encoding="utf-8")

print(f"\n Scraped {len(df)} route pages successfully")
print(df.head(10))

df = pd.read_csv("daewoo_full_routes.csv")
mask = df["url"].str.contains("timing|schedule|route|lahore|islamabad|karachi|multan|faisalabad", case=False, na=False)
df_filtered = df[mask].copy()
def extract_route(title):
    if not isinstance(title, str):
        return (None, None)
    match = re.search(r"([A-Za-z\s]+)\s+(?:to|‚Äì|-)\s+([A-Za-z\s]+)", title)
    if match:
        return (match.group(1).strip(), match.group(2).strip())
    return (None, None)

df_filtered[["origin_clean", "destination_clean"]] = df_filtered.apply(
    lambda row: pd.Series(extract_route(row["origin"] or "")), axis=1
)
df_filtered = df_filtered[df_filtered["details"].str.contains("am|pm|departure|arrival|duration|hours", case=False, na=False)]

df_filtered.to_csv("daewoo_clean_routes.csv", index=False, encoding="utf-8")

print(f"Filtered to {len(df_filtered)} real route/timing pages")
print(df_filtered[["url", "origin_clean", "destination_clean", "details"]].head(10))

# Load filtered data
df = pd.read_csv("daewoo_clean_routes.csv")

# Step 1: Define known Daewoo city list
cities = ["Lahore", "Islamabad", "Rawalpindi", "Multan", "Karachi", "Faisalabad", "Peshawar", "Sahiwal", "Gujranwala"]

# Step 2: Extract origin city from URL or title
def extract_city_from_url(url):
    for city in cities:
        if city.lower() in str(url).lower():
            return city
    return None

df["origin_city"] = df["url"].apply(extract_city_from_url)

# Step 3: Detect destination mentions inside text
def detect_destinations(text, origin):
    mentioned = [c for c in cities if c.lower() in str(text).lower() and c != origin]
    return ", ".join(mentioned) if mentioned else None

df["destinations"] = df.apply(lambda row: detect_destinations(row["details"], row["origin_city"]), axis=1)

# Step 4: Keep rows with at least one city found
df_final = df[df["origin_city"].notna()].copy()

# Step 5: Save new enriched dataset
df_final.to_csv("daewoo_city_links.csv", index=False, encoding="utf-8")

print(f"Found {len(df_final)} pages with identifiable cities")
print(df_final[["url", "origin_city", "destinations"]].head(10))

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta

# Load your previously created city-links file
df = pd.read_csv("daewoo_city_links.csv")

# Define some helper data
time_slots = [
    "05:00", "07:00", "09:00", "11:00", "13:00",
    "15:00", "17:00", "19:00", "21:00", "23:00"
]
days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
weather_conditions = ["Sunny", "Cloudy", "Rainy", "Foggy", "Windy"]

# Estimated base travel times (in minutes) between major cities (fictional but realistic)
base_durations = {
    ("Lahore", "Islamabad"): 270,
    ("Lahore", "Faisalabad"): 120,
    ("Lahore", "Multan"): 300,
    ("Lahore", "Karachi"): 720,
    ("Islamabad", "Peshawar"): 180,
    ("Karachi", "Multan"): 660,
    ("Karachi", "Lahore"): 720,
    ("Faisalabad", "Multan"): 240,
    ("Multan", "Rawalpindi"): 300,
}

synthetic_data = []

# Generate synthetic trips
for _, row in df.iterrows():
    origin = row["origin_city"]
    if pd.isna(row["destinations"]):
        continue
    destinations = [d.strip() for d in str(row["destinations"]).split(",")]

    for dest in destinations:
        # Get estimated duration (base)
        base_time = base_durations.get((origin, dest)) or base_durations.get((dest, origin)) or random.randint(180, 600)

        for _ in range(50):  # generate 50 random trips per route
            dep_time_str = random.choice(time_slots)
            dep_dt = datetime.strptime(dep_time_str, "%H:%M")

            # Calculate scheduled arrival
            arr_dt = dep_dt + timedelta(minutes=base_time)
            arr_time_str = arr_dt.strftime("%H:%M")

            # Simulate a delay
            delay = int(np.random.normal(loc=20, scale=15))  # avg delay = 20 ¬± 15 min
            delay = max(0, delay)  # no negative delays
            actual_duration = base_time + delay

            # Choose a random day and weather
            day = random.choice(days)
            weather = random.choice(weather_conditions)

            synthetic_data.append({
                "Origin": origin,
                "Destination": dest,
                "Departure": dep_time_str,
                "Arrival": arr_time_str,
                "Scheduled_Duration(min)": base_time,
                "Actual_Duration(min)": actual_duration,
                "Delay(min)": delay,
                "Day": day,
                "Weather": weather
            })

# Create DataFrame
delay_df = pd.DataFrame(synthetic_data)

# Save dataset
delay_df.to_csv("synthetic_daewoo_delay_data.csv", index=False, encoding="utf-8")

print(f"‚úÖ Generated {len(delay_df)} synthetic trips across {delay_df['Origin'].nunique()} cities")
print(delay_df.head(10))

df = pd.read_csv("synthetic_daewoo_delay_data.csv")
print(df.info())
print(df.head())

# Quick summary
print("\nAverage delay by origin:")
print(df.groupby("Origin")["Delay(min)"].mean())

from sklearn.preprocessing import LabelEncoder

enc = LabelEncoder()
for col in ["Origin", "Destination", "Day", "Weather"]:
    df[col] = enc.fit_transform(df[col])

df.head()

X = df[["Origin", "Destination", "Scheduled_Duration(min)", "Departure", "Day", "Weather"]]
y = df["Delay(min)"]

# Convert Departure (HH:MM) to minutes since midnight
X["Departure"] = X["Departure"].apply(lambda t: int(t.split(":")[0])*60 + int(t.split(":")[1]))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("MAE:", mean_absolute_error(y_test, y_pred))
print("R¬≤ Score:", r2_score(y_test, y_pred))

import matplotlib.pyplot as plt

imp = pd.Series(model.feature_importances_, index=X.columns)
imp.sort_values(ascending=True).plot(kind="barh", figsize=(8,5), title="Feature Importance")
plt.show()

import seaborn as sns
sns.boxplot(x="Day", y="Delay(min)", data=df)

rch

# ===============================
# üöå Daewoo Bus Delay Predictor
# ===============================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import datetime as dt

# 1Ô∏è‚É£ Load your synthetic dataset
df = pd.read_csv("synthetic_daewoo_delay_data.csv")

# 2Ô∏è‚É£ Encode categorical variables
encoders = {}
for col in ["Origin", "Destination", "Day", "Weather"]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoders[col] = le

# Convert Departure time (HH:MM) ‚Üí minutes since midnight
df["Departure"] = df["Departure"].apply(lambda t: int(t.split(":")[0])*60 + int(t.split(":")[1]))

# 3Ô∏è‚É£ Split into features (X) and target (y)
X = df[["Origin", "Destination", "Scheduled_Duration(min)", "Departure", "Day", "Weather"]]
y = df["Delay(min)"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4Ô∏è‚É£ Train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5Ô∏è‚É£ Evaluate basic performance
y_pred = model.predict(X_test)
print("MAE:", mean_absolute_error(y_test, y_pred))
print("R¬≤ Score:", r2_score(y_test, y_pred))

# =========================================
# üß† Real-time User Prediction
# =========================================

# Take user input
print("\nüöå === Delay Prediction Input Form ===")
origin_input = input("Enter Origin City (e.g., Lahore): ").title()
destination_input = input("Enter Destination City (e.g., Islamabad): ").title()
date_input = input("Enter Travel Date (YYYY-MM-DD): ")
time_input = input("Enter Departure Time (HH:MM, 24h format): ")

import requests

# ------------- üå¶Ô∏è AUTO WEATHER DETECTION -------------
def get_weather_condition(city, date):
    """
    Fetch forecast from Open-Meteo API (free, no key).
    Returns simplified condition: 'Clear', 'Rainy', or 'Foggy'.
    """
    # Map major cities to lat/lon (expand as needed)
    city_coords = {
        "Lahore": (31.5204, 74.3587),
        "Islamabad": (33.6844, 73.0479),
        "Karachi": (24.8607, 67.0011),
        "Multan": (30.1575, 71.5249),
        "Faisalabad": (31.418, 73.079),
        "Peshawar": (34.0151, 71.5249),
        "Quetta": (30.1798, 66.975),
        "Rawalpindi": (33.6, 73.033),
        "Sialkot": (32.5, 74.5),
        "Jhang": (31.268, 72.318)
    }

    if city not in city_coords:
        return "Clear"  # fallback if city not listed

    lat, lon = city_coords[city]
    url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&daily=weathercode&start_date={date}&end_date={date}&timezone=auto"
    response = requests.get(url)
    data = response.json()

    try:
        code = data["daily"]["weathercode"][0]
    except (KeyError, IndexError):
        return "Clear"

    # Map weather codes to readable labels
    if code in [0, 1]: return "Clear"
    elif code in [2, 3, 45, 48]: return "Foggy"
    elif code in [51, 53, 55, 61, 63, 65, 80, 81, 82]: return "Rainy"
    else: return "Clear"

# Fetch automatically
weather_input = get_weather_condition(origin_input, date_input)
print(f"üå¶Ô∏è Weather detected for {origin_input} on {date_input}: {weather_input}")


# Extract weekday from date
day_name = dt.datetime.strptime(date_input, "%Y-%m-%d").strftime("%A")

# Prepare features
try:
    origin_val = encoders["Origin"].transform([origin_input])[0]
except ValueError:
    origin_val = 0  # unseen city fallback

try:
    dest_val = encoders["Destination"].transform([destination_input])[0]
except ValueError:
    dest_val = 0

try:
    day_val = encoders["Day"].transform([day_name])[0]
except ValueError:
    day_val = 0

try:
    weather_val = encoders["Weather"].transform([weather_input])[0]
except ValueError:
    weather_val = 0

dep_minutes = int(time_input.split(":")[0]) * 60 + int(time_input.split(":")[1])

# Pick average scheduled duration (you can refine this later)
avg_duration = df["Scheduled_Duration(min)"].mean()

# Create feature row
input_features = [[origin_val, dest_val, avg_duration, dep_minutes, day_val, weather_val]]

# Predict delay
predicted_delay = model.predict(input_features)[0]

print("\nüö¶ Predicted Delay:", round(predicted_delay, 2), "minutes")

# Suggestion based on threshold
if predicted_delay < 10:
    print("On-time or minimal delay expected.")
elif predicted_delay < 30:
    print("Moderate delay expected. Consider leaving earlier.")
else:
    print("Significant delay expected. You may want to reschedule.")

df.head(5)

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import joblib

# Example dataset
df = pd.read_csv("synthetic_daewoo_delay_data.csv")

# Example feature columns and target
X = df[["Origin", "Destination", "Duration", "DepartureTime", "Day", "Weather"]]
y = df["DelayMinutes"]

# Encode categorical columns (for simplicity here)
from sklearn.preprocessing import LabelEncoder
encoders = {}
for col in ["Origin", "Destination", "Day", "Weather"]:
    encoders[col] = LabelEncoder()
    X[col] = encoders[col].fit_transform(X[col])

# Train model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# Save model and encoders
joblib.dump(model, "model.pkl")
joblib.dump(encoders, "encoders.pkl")

print("‚úÖ Model and encoders saved successfully!")

